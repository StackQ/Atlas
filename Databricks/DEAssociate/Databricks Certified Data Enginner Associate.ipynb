{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Databricks** : Multi-cloud Lakehouse platform based on Apache Spark\n",
    "\n",
    "- Lakehouse : Data Lake + Data Warehouse : One platform that unify all of our data enginnering, analytics and AI workloads\n",
    "    - Data Lake\n",
    "        - Open\n",
    "        - Flexible\n",
    "        - ML Support\n",
    "    - Data Warehouse\n",
    "        - Reliable\n",
    "        - Strong governance\n",
    "        - Performance\n",
    "\n",
    "- Lakehouse Platform Architecture : Cloud Service + Runtime + Workspace\n",
    "    - Cloud Service\n",
    "        - Azure\n",
    "        - AWS\n",
    "        - GCP\n",
    "    - Runtime\n",
    "        - Apache Spark\n",
    "        - Delta Lake\n",
    "    - Workspace\n",
    "        - Data Engineering\n",
    "        - Data Warehousing\n",
    "        - Machine Learning\n",
    "\n",
    "- Lakehouse Platform : Control Plant + Data Plane : this is how databricks resources are deplyed in our cloud provider, when ever we create a databricks workspace, it is deployed in the control plane along with databricks services like databricks UI, cluster mgmt, workflows & notebooks\n",
    "    - Control Plane- Resides in Databricks account, provided by DBricks need to use and control our infrastructure(data plane).\n",
    "        - Web UI\n",
    "        - Cluster Management\n",
    "        - Workflows\n",
    "        - Notebooks\n",
    "    - Data Plane- resides/deployed in our own cloud subscription, so compute & storage will be always in our own cloud account.\n",
    "        - Cluster VMs\n",
    "        - Sotrage(DBFS)- just an abstraction layer, while it uses the underlying cloud storage to persist the data. Whenever we create a cluster in Databricks, it comes pre-installed with DBFS. We use file systems to persist data and files. if we create a file in our cluster and store it in DBFS, this file is actually persisted in the underlying cloud storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Lakehouse Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta Lake\n",
    "\n",
    "Advantages  \n",
    "    - Brings ACID tran to object storage  \n",
    "    - Handle scalable metadata   \n",
    "    - Full audit trail of all changes  \n",
    "    - Builds upon standard data formats: Parquet + Json  \n",
    "\n",
    "Open source storage framework that brings reliability to *DATA LAKES*. *DATA LALES* have many limitations, such as data inconsistency and performance issues. **Delta Lake** technology helps overcoming these challenges.\n",
    "\n",
    "**Delta Lake is/is not**\n",
    "\n",
    "| is                         | is not                          |\n",
    "|----------------------------|---------------------------------|\n",
    "| Open-source technology     | proprietary technology |\n",
    "| Storage framework/layer    | Storage format/medium |\n",
    "| Enabling building lakehouse| Data warehouse/Database service|\n",
    "\n",
    "Delta lake is a component which is deployed on the cluster as part of the Databricks runtime. Creating a Delta Lake table, it gets stored on the storage in one or more data files in parquet format. But along with these files, Delta stores a transaction log as well.\n",
    "\n",
    "*Delta Table* = Clusetr(Delta Lake) + Storage(Parquet Files + Transaction/Delta log .json)\n",
    "\n",
    "Transaction/Delta Log = Ordered records of every transaction performed on the table since its creation. It serves as a single source of truth. So every time we query the table, Spark checks this transaction log to retrieve the most recent version of the data.\n",
    "\n",
    "JSON File = contains commit information:  \n",
    "    - Operation performed + predicates used\n",
    "        - `{\"commitInfo\":{\"timestamp\":1722518153427,\"userId\":\"8480822684007256\",\"userName\":\"d5ac5b60-a9ca-4d50-b100-4abbce85c97b\",\"operation\":\"OPTIMIZE\",\"operationParameters\":{\"predicate\":\"[]\",\"zOrderBy\":\"[]\",\"batchId\":\"0\",\"auto\":false},\"job\":{\"jobId\":\"99802076724030\",\"jobName\":\"rich_web_events\",\"runId\":\"226193597880476\",\"jobOwnerId\":\"8480822684007256\",\"triggerType\":\"manual\"},\"clusterId\":\"0801-121041-vppzjxir\",\"readVersion\":1604,\"isolationLevel\":\"SnapshotIsolation\",\"isBlindAppend\":false,\"operationMetrics\":{\"numRemovedFiles\":\"25520\",\"numRemovedBytes\":\"97541851305\",\"p25FileSize\":\"1054520126\",\"numDeletionVectorsRemoved\":\"0\",\"minFileSize\":\"229412771\",\"numAddedFiles\":\"93\",\"maxFileSize\":\"1094755631\",\"p75FileSize\":\"1091163815\",\"p50FileSize\":\"1079090452\",\"numAddedBytes\":\"97664988674\"},\"engineInfo\":\"Databricks-Runtime/12.2.x-scala2.12\",\"txnId\":\"15228374-5984-4d81-a09d-3a4668ba2e5a\"}}`     \n",
    "    - data files affected(added/removed)  \n",
    "        - Removed `{\"remove\":{\"path\":\"date=2024-07-29/part-00003-b6bd805a-84a3-4634-b6de-fa3a77f536f1.c000.snappy.parquet\",\"deletionTimestamp\":1722517274977,\"dataChange\":false,\"extendedFileMetadata\":true,\"partitionValues\":{\"date\":\"2024-07-29\"},\"size\":4897173,\"tags\":{\"INSERTION_TIME\":\"1722517118000012\",\"MIN_INSERTION_TIME\":\"1722517118000012\",\"MAX_INSERTION_TIME\":\"1722517118000012\",\"OPTIMIZE_TARGET_SIZE\":\"1073741824\"}}}`  \n",
    "        -Add `{\"add\":{\"path\":\"date=2024-08-01/part-00092-cc1a7030-d615-48e8-bd0b-a10288ae21f1.c000.snappy.parquet\",\"partitionValues\":{\"date\":\"2024-08-01\"},\"size\":229412771,\"modificationTime\":1722517314000,\"dataChange\":false,\"stats\":\"{\\\"numRecords\\\":236070,\\\"minValues\\\":{\\\"segio_schema\\\":\\\"admin_xxxx_com\\\",\\\"received_at\\\":\\\"2024-08-01 00:00:00.060\\\",\\\"sent_at\\\":\\\"2024-05-19 11:14:13.258\\\",\\\"first_anonymous_id\\\":\\\"\\\\\\\"\\\\\\\"3830790f-0f3d-4069-b246-f4fc87\\\",\\\"anonymous_id\\\":\\\"\\\\\\\"\\\\\\\"3830790f-0f3d-4069-b246-f4fc87\\\",\\\"id\\\":\\\"00013afc-932f-4aef-bc71-1b3e6869\\\",\\\"url\\\":\\\"http://xxxxx.com/engineering\\\",\\\"actual_url\\\":\\\"http:///\\\",\\\"page\\\":\\\"ace.xxxxxx.com/events/details\\\",\\\"hostname\\\":\\\"ace.xxxxx.com\\\",\\\"referrer_hostname\\\":\\\"0.0.0.0\\\",\\\"clean_path\\\":\\\"\\\",\\\"referrer_clean_path\\\":\\\"\\\",\\\"referrer\\\":\\\"\\\",\\\"channel\\\":\\\"direct\\\",\\\"channel_validation\\\":\\\"['direct']\\\",\\\"non_reportable\\\":0,\\\"new_user\\\":0,\\\"session_id\\\":\\\"\\\\\\\"\\\\\\\"3830790f-0f3d-4069-b246-f4fc87\\\",\\\"session\\\":1,\\\"non_page_session\\\":1,\\\"bounce\\\":0,\\\"exit\\\":0,\\\"campaign_name\\\":\\\"\\\",\\\"campaign_source\\\":\\\"\\\",\\\"campaign_medium\\\":\\\"\\\",\\\"campaign_content\\\":\\\"\\\",\\\"mid\\\":\\\"\\\",\\\"ad_groupname\\\":\\\"P:access|O:ppm|V:google|g:emea|L\\\",\\\"network_type\\\":\\\"\\\",\\\"advertised_product\\\":\\\"xxxxx\\\"},\\\"maxValues\\\":{\\\"segio_schema\\\":\\\"university_xxxxx_com\\\",\\\"received_at\\\":\\\"2024-08-01 10:36:41.464\\\",\\\"sent_at\\\":\\\"2024-08-01 22:59:34.620\\\",\\\"first_anonymous_id\\\":\\\"sp-null\\\",\\\"anonymous_id\\\":\\\"sp-null\\\",\\\"id\\\":\\\"node-next-1722507906130-23498a3c�\\\",\\\"url\\\":\\\"https://www.statuspage.io/pricin�\\\",\\\"actual_url\\\":\\\"https://www.xxxx.com/en�\\\",\\\"page\\\":\\\"www.statuspage.io/statuspage/pri�\\\",\\\"hostname\\\":\\\"www.statuspage.io\\\",\\\"referrer_hostname\\\":\\\"zumu.xxxx.net\\\",\\\"clean_path\\\":\\\"/zh/work-management/team-managem�\\\",\\\"referrer_clean_path\\\":\\\"/zssd/oe_field_suggestions/pull-�\\\",\\\"referrer\\\":\\\"https://zumu.xxxxx.net/\\\",\\\"channel\\\":\\\"unpaid-video\\\",\\\"channel_validation\\\":\\\"['unpaid-video']\\\",\\\"non_reportable\\\":1,\\\"new_user\\\":1,\\\"session_id\\\":\\\"sp-null:2024-08-01 00:12:25.698\\\",\\\"session\\\":1347,\\\"non_page_session\\\":1076,\\\"bounce\\\":1,\\\"exit\\\":1,\\\"campaign_name\\\":\\\"www.saltedgestatus.com\\\",\\\"campaign_source\\\":\\\"youtube,gdn\\\",\\\"campaign_medium\\\":\\\"website\\\",\\\"campaign_content\\\":\\\"zenaptix\\\",\\\"mid\\\":\\\"\\\",\\\"ad_groupname\\\":\\\"p:jwm|O:ppm|V:google|G:us|L:en|F�\\\",\\\"network_type\\\":\\\"ytv\\\",\\\"advertised_product\\\":\\\"not-advertised\\\"},\\\"nullCount\\\":{\\\"segio_schema\\\":175284,\\\"received_at\\\":0,\\\"sent_at\\\":0,\\\"first_anonymous_id\\\":0,\\\"anonymous_id\\\":0,\\\"id\\\":0,\\\"url\\\":187316,\\\"actual_url\\\":73038,\\\"page\\\":187316,\\\"hostname\\\":187316,\\\"referrer_hostname\\\":181037,\\\"clean_path\\\":0,\\\"referrer_clean_path\\\":0,\\\"referrer\\\":73519,\\\"channel\\\":0,\\\"channel_validation\\\":0,\\\"non_reportable\\\":0,\\\"new_user\\\":0,\\\"session_id\\\":0,\\\"session\\\":0,\\\"page_session\\\":236070,\\\"non_page_session\\\":0,\\\"bounce\\\":0,\\\"exit\\\":0,\\\"campaign_name\\\":49,\\\"campaign_source\\\":0,\\\"campaign_medium\\\":0,\\\"campaign_content\\\":0,\\\"mid\\\":0,\\\"ad_groupname\\\":233905,\\\"network_type\\\":232072,\\\"advertised_product\\\":0}}\",\"tags\":{\"MAX_INSERTION_TIME\":\"1722517118025519\",\"INSERTION_TIME\":\"1722517118024991\",\"MIN_INSERTION_TIME\":\"1722517118024991\",\"OPTIMIZE_TARGET_SIZE\":\"1073741824\"}}}`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CASE 1 Writes/Reads**\n",
    "\n",
    "*Writer Process & Reader Prcoess* ::\n",
    "Once the writer process starts, it stores the detla lake table in 2 data files in parquet format. As soon as the writer process finishes writing, it adss the transaction log (000.json & 000.crc) into the _delta_log dir along with latest transaction json number and it's info in file **_last_checkpoint**.\n",
    "\n",
    "A reader process always starts by reading a transaction log.\n",
    "\n",
    "Data Files ::  \n",
    "|File1.parquet|File2.parquet|\n",
    "|-------------|-------------|\n",
    "\n",
    "_delta_log ::  _last_checkpoint | (000.json, 000.crc )\n",
    "\n",
    ">>  Write data as below -> Add transaction log ->  Read tran log(000.json(holds file names File1 & File2.parquet), since 000.json present in _last_checkpoint) as `{\"version\":000,\"size\":53666,\"parts\":2,\"sizeInBytes\":5928790,\"numOfAddFiles\":6917,\"checkpointSchema\":...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CASE 2 UPDATES**\n",
    "\n",
    "*Writer Process & Reader Prcoess* :: In this scenario, the writer process wants to update a record which presents in the File1.parquet. But in delta lake instead of updating the record in the file itself, it will make a copy of this file and make the necessary updates in the new file. it then updates the log by writing a new JSON file 001.json & 001.crc file. This new log file knows that file number 1 is no longer needed. now the READER process reads the transaction log that tells that only files 2 & 3 are part of the current table version so it can start reading them.\n",
    "\n",
    "A reader process always starts by reading a transaction log.\n",
    "\n",
    "Data Files ::  \n",
    "|File2.parquet| X File1.parquet copy & update to -> |File3.parquet|\n",
    "|-------------|-------------|------|\n",
    "\n",
    "_delta_log ::  _last_checkpoint | (000.json, 000.crc) | (001.json, 001.crc) \n",
    "\n",
    ">>  Writer Update data -> Add transaction log 001.json ->  Read tran log(001.json(holds file names File2 & File3.parquet), since 001.json present in _last_checkpoint) as `{\"version\":001,\"size\":54666,\"parts\":2,\"sizeInBytes\":5928790,\"numOfAddFiles\":6917,\"checkpointSchema\":..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CASE 3 Simultaneous Writes/Reads**\n",
    "\n",
    "*Writer Process & Reader Prcoess* :: Here both processes want to work at the same time. The writer process starts writing the File4.parquet. On the other hand, the READER process reads the transaction log that only has information about files 2 & 3 and not file 4 as it is not fully written yet. So it starts reading those files, 2&3 which represent the most recent data at the moment. so Delta lake guarantees that we will always get the most recent version of the data. Read operation will never have a deadlock state or conflicts with any ongoing operation on the table. Finally writer process finishes and it adds a new file to the log.\n",
    "\n",
    "\n",
    "**CASE 3 Failed Writes**  \n",
    "*Writer Process & Reader Prcoess* :: WRITER process starts writing the file number 5 to the lake, but this time there is an error in the job, which leads to adding an incomplete file. coz of this failure, Delta lake module does not write any information to the log. No the READER process reads the tran log that has no information about that incomplete file File5.parquet. That's why the READER process will read only file 2,3,4 only. So Delta lake guarantees that we will never read dirty data. So the tran log is the magic for ACID.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features\n",
    "\n",
    "- Time Travel\n",
    "    - SELECT * FROM schema.emp TIMESTAMP AS OF \"2024-01-01\"\n",
    "    - SELECT * FROM schema.emp VERSION AS OF 10\n",
    "    - SELECT * FROM schema.emp@v9\n",
    "    - RESTORE TABLE schema.emp TO TIMESTAMP AS OF \"2023-12-01\"\n",
    "    - RESTORE TABLE schema.emp TO VERSION AS OF 09\n",
    "- Compacting Small Files and Indexing\n",
    "    - OPTMIZE schema.emp : many samll files will be compacted in one or more larger files to improve table perf.\n",
    "    - OPTMIZE schema.emp ZORDER BY emp_id\n",
    "        - File1(1-99) | File2(55-10) | File3(15-50) | File4(75-30) | File5(8-100) | File6(44-56) will be as below after ZORDER by emp_id\n",
    "        - assume ZORDER BY emp_id :: File1(1-50) | File2(51-100) : Z Order indexing is used by *DATA SKIPPING* algorithm to extremly reduce the amount of data that need to be read. Here if we query an ID 11, Delta is sure now 11 is in File1, so it can skip scanning the File2 which will save a huge amout of time.\n",
    "- Vacuum\n",
    "    - UnCommited files\n",
    "    - Files that are no longer in latest table state\n",
    "    - VACUUM schema.emp [retention period] -> VACUUM schema.emp [15] | Default Retention period: 7 days\n",
    "    - VACUUM schema.emp RETAIN 0 HOURS, will not work coz default threshold 7 days\n",
    "    - SET spark.databricks.retentionDurationCheck.enabled = false;\n",
    "    - Just to be sure that no longer running operations are still ref any of the files to be deleted.\n",
    "    - once ran a VACUUM on delta table, lose the ability to time and travel back to a version older than the specified retention period, coz the data files are no longer exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CTAS**\n",
    "\n",
    "CREATE TABLE new_table\n",
    "COMMENT \"CTAS SYNTAX with custom location & partitioned by\"\n",
    "PARTITIONED BY (city, PAN)\n",
    "LOCATION '/some/new/path'\n",
    "AS\n",
    "SELECT id AS citizen_id, CONCAT(FName, \" \", LName) AS Full_Name, DOB, PAN, city FROM india.citizens\n",
    "\n",
    "**Table Constraints**\n",
    "Currently supports 2 types of table constraints\n",
    "- NOT NULL\n",
    "- CHECK : ALTER TABLE schema.emp ADD CONSTRAINT valid_DOB CHECK (DOB > '1980-01-01')\n",
    "\n",
    "**Cloning Delta Lake tables**\n",
    "- DEEP Clone : Full copies data + metadata from a source table to a target\n",
    "    - CREATE TABLE new_table DEEP CLONE source_table\n",
    "    - can sync changes : executing above command again can sync chagnes from source to the target location.\n",
    "    - Takes more time\n",
    "- SHALLOW Clone\n",
    "    - Quickly create a copy of a table : just copy the delta tran logs\n",
    "    - CREATE TABLE new_table SHALLOW CLONE source_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Views**\n",
    "\n",
    "- Classical view/Stored View : like table stored views are persisted in the database. definition is stored but not the data itself\n",
    "    - CREATE VIEW view_name AS query\n",
    "    - DROP VIEW view_name\n",
    "- Temp \n",
    "    - Dropped when session ends, Session-scoped\n",
    "    - CREATE TEMP VIEW view_name AS query\n",
    "    - When Spark session created in DBricks\n",
    "        - Opening a new notebook\n",
    "        - Detaching and reattaching to a cluster\n",
    "        - Installing a python package\n",
    "        - Restarting a cluster\n",
    "- Global Temp Views\n",
    "    - Cluster-scoped views, dropped when cluster restarted\n",
    "    - CREATE GLOBAL TEMP VIEW view_name AS query -> SELECT * FROM global_temp.view_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELT with Spark SQL & Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Querying Files**\n",
    "\n",
    "- SELECT * FROM file_format.\\`/path/to/file\\`\n",
    "    - file_format\n",
    "        - self-describing formats\n",
    "            - json\n",
    "            - parquet\n",
    "        - non self-describing formats\n",
    "            - csv\n",
    "            - tsv\n",
    "    - \\`/path/to/file\\`\n",
    "        - single file: part0001.json\n",
    "        - multiple file: part_*.json\n",
    "        - complete dir: /path/dir\n",
    "    - examples\n",
    "        - select * from json.\\`/path/file_name.json\\`\n",
    "    - extract text file as raw strings\n",
    "        - text-based files (JSON, CSV, TSV and TXT formats)\n",
    "            - SELECT * FROM **text**.\\`/path/to/file\\`\n",
    "    - extract files as raw bytes\n",
    "        - images or unstructured data\n",
    "        - SELECT * FROM **binaryFile**.\\`/path/to/file\\`\n",
    "\n",
    "**CTAS**: Registering Tables from files\n",
    "\n",
    "- CREATE TABLE table_name AS SELECT * FROM file_format.\\`/path/to/file\\`\n",
    "    - Automatically infer schema information from query results\n",
    "        - Do not support manual schema declaration\n",
    "        - Useful for external data ingestion with well-defined schema\n",
    "    - Do not support file options : using delta/csv/text/tsv.....any\n",
    "    - External table\n",
    "        CRATE TABLE table_name (cols datatype) USING JDBC OPTIONS(url=\"jdbc:sqlite://hostname:port\", dbtable=\"db.table\", user=\"\", password=\"pwd)\n",
    "            - it's not delta table\n",
    "            - can't expect the perf guarantees associated with Delta Lake and Lakehouse\n",
    "            - Having a huge database table\n",
    "- Create delta table using non-delta file format/external data source : Create temp view referring to external data source and then CTAS using temp view  \n",
    "    1- CREATE TEMP VIEW tmp_vw (cols dtypes) USING data_source OPTIONS(key1=val1, key2=val2)  \n",
    "    2- CREATE TABLE table_name AS SELECT * FROM tmp_vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT *, input_file_name() source_file FROM delta.`path/to/file`\n",
    "\n",
    "--Allows to query any text based file\n",
    "SELECT * FROM text.`/path/to/file_json`\n",
    "\n",
    "# will give single column with header as first row, data will be not parsed\n",
    "CREATE TABLE csv_table_airlines AS SELECT * FROM csv.'/path/to/csv_files'\n",
    "\n",
    "#Managed\n",
    "CREATE TEMP VIEW tmp_vw_airlines(flight str, source str, dest str)\n",
    "USING CSV\n",
    "OPTIONS(\n",
    "    path=\"/path/to/csv/csv_files_*.csv\",\n",
    "    header=\"true\",\n",
    "    delimiter=\",\"\n",
    ");\n",
    "\n",
    "CREATE TABLE airlines AS SELECT * FROM tmp_vw_airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRAS\n",
    "CRATE OR REPLACE TABLE table_name \n",
    "SELECT * FROM parquet.`/path/to/parquet_file`\n",
    "\n",
    "#Insert Overwrites: diff with above SQL is, can only override the new records that match the current table schema, which means that it is \n",
    "#a safer technique for overwriting an existing table without the risk of modifying the table schema\n",
    "INSERT OVERWRITE table_name SELECT * FROM parquet.`/path/to/parquet_file`\n",
    "\n",
    "#will generate exception as current_time column won't be present in table schema\n",
    "INSSERT OVERWRITE table_name SELECT *, current_time() FROM parquet.`/path/to/parquet_file`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced Transformations**\n",
    "\n",
    "- Col profile(str) values: {\"fname\":\"\", \"lname\":\"\", \"gender\":\"\", \"address\":{\"distt\":\"\",\"state\":\"\",\"country\":\"\"}}\n",
    "- Spark has built-in feature to directly interact with JSON data stored as string\n",
    "    - `SELECT profile:fname, profile:address:state FROM table_name`\n",
    "- Spark sql has ability to parse JSON objects into struct type\n",
    "    - `SELECT from_json(profile) AS profile_struct FROM table_name`\n",
    "- explode()\n",
    "- collect_set(): to collect unique values for a field, indicating fields within arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW parsed_table_vw AS \n",
    "SELECT \n",
    "from_json(profile, schema_of_json('{\"fname\":\"\", \"lname\":\"\", \"gender\":\"\", \"address\":{\"distt\":\"\",\"state\":\"\",\"country\":\"\"}}')) profile_struct\n",
    "FROM table_name\n",
    "\n",
    "SELECT * FROM parsed_table_vw\n",
    "\n",
    "SELECT profile_struct.fname, profile_struct.address.country FROM parsed_table_vw\n",
    "\n",
    "#Once json string is converted to a struct type, can use * operation to flatten fields into columns\n",
    "SELECT profile_struct.* FROM parsed_table_vw\n",
    "\n",
    "#---------------------Executed SQL Example for above---------------------------\n",
    "CREATE OR REPLACE TEMP VIEW parsed_table_profile AS \n",
    "SELECT \n",
    "  id, \n",
    "  from_json(profile, schema_of_json('{\"fname\":\"\", \"lname\":\"\", \"gender\":\"\", \"address\":{\"distt\":\"\",\"state\":\"\",\"country\":\"\"}}')) profile_struct\n",
    "FROM (VALUES\n",
    "  (1, '{\"fname\":\"Raj\", \"lname\":\"Shukla\", \"gender\":\"Male\", \"address\":{\"distt\":\"xyz\",\"state\":\"MH\",\"country\":\"India\"}}'),\n",
    "  (2, '{\"fname\":\"Sridhar\", \"lname\":\"Iyer\", \"gender\":\"Male\", \"address\":{\"distt\":\"abc\",\"state\":\"KA\",\"country\":\"India\"}}'))AS T(id, profile)\n",
    "\n",
    "#select from struct\n",
    "SELECT profile_struct.fname, profile_struct.address.country FROM parsed_table_profile\n",
    "\n",
    "#flat all values\n",
    "SELECT profile_struct.* FROM parsed_table_profile \n",
    "\n",
    "#-------------------explode-------\n",
    "SELECT id, order_id, book FROM (\n",
    "  VALUES \n",
    "  ('C001', 'O001', ARRAY(map(\"id\",\"B01\", \"qty\",\"5\", \"ttl\",\"50\"))),\n",
    "  ('C002', 'O002', ARRAY(map(\"id\",\"B02\", \"qty\",\"6\", \"ttl\",\"60\"), map(\"id\",\"B03\", \"qty\",\"3\", \"ttl\",\"30\")))  \n",
    ")AS T(id, order_id, book)\n",
    "\n",
    "SELECT id, explode(book) FROM (\n",
    "  VALUES \n",
    "  ('C001', 'O001', ARRAY(map(\"id\",\"B01\", \"qty\",\"5\", \"ttl\",\"50\"))),\n",
    "  ('C002', 'O002', ARRAY(map(\"id\",\"B02\", \"qty\",\"6\", \"ttl\",\"60\"), map(\"id\",\"B03\", \"qty\",\"3\", \"ttl\",\"30\")))  \n",
    ")AS T(id, order_id, book)\n",
    "\n",
    "#---------collect_set--------\n",
    "SELECT id, collect_set(order_id) AS orders_set, collect_set(book.id) \n",
    "FROM\n",
    "(SELECT id, order_id, explode(book) book FROM (\n",
    "  VALUES \n",
    "  ('C001', 'O001', ARRAY(map(\"id\",\"B01\", \"qty\",\"5\", \"ttl\",\"50\"))),\n",
    "  ('C002', 'O002', ARRAY(map(\"id\",\"B02\", \"qty\",\"6\", \"ttl\",\"60\"), map(\"id\",\"B03\", \"qty\",\"3\", \"ttl\",\"30\")))  \n",
    ")AS T(id, order_id, book)\n",
    ")\n",
    "GROUP BY id\n",
    "\n",
    "#------------------Flatten & distinct--------------------\n",
    "SELECT id, order_id, books, array_distinct(flatten(books)) FROM (\n",
    "  VALUES \n",
    "  ('C001', 'O001', ARRAY(array(\"B01\",\"B02\"), array(\"B01\",\"B09\"), array(\"B12\",\"B13\"))),\n",
    "  ('C002', 'O002', ARRAY(array(\"B04\",\"B06\"), array(\"B02\",\"B06\", \"B01\"), array(\"B07\",\"B06\")))\n",
    ")AS T(id, order_id, books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Higher Order Functions and SQL UDFs**\n",
    "\n",
    "SELECT id, order_id, qty, book FROM (\n",
    "  VALUES \n",
    "  ('C001', 'O001', 48, ARRAY(map(\"id\",\"B01\", \"qty\",\"5\", \"ttl\",\"50\"))),\n",
    "  ('C002', 'O002', 56, ARRAY(map(\"id\",\"B02\", \"qty\",\"6\", \"ttl\",\"60\"), map(\"id\",\"B03\", \"qty\",\"3\", \"ttl\",\"30\")))  \n",
    ")AS T(id, order_id, qty, book)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------Filter on array values----------------------\n",
    "WITH C AS \n",
    "(\n",
    "  SELECT id, order_id, qty, books FROM (\n",
    "  VALUES \n",
    "  ('C001', 'O001', 48, ARRAY(map(\"id\",\"B01\", \"qty\",\"5\", \"ttl\",\"50\"))),\n",
    "  ('C002', 'O002', 56, ARRAY(map(\"id\",\"B02\", \"qty\",\"6\", \"ttl\",\"60\"), map(\"id\",\"B03\", \"qty\",\"3\", \"ttl\",\"30\")))  \n",
    "  )AS T(id, order_id, qty, books)\n",
    ")\n",
    "SELECT order_id, books, FILTER(books, i -> i.qty >=4) AS multiple_copies\n",
    "FROM C\n",
    "\n",
    "#---------------------------------------size only with subquery----------------------\n",
    "WITH C AS \n",
    "(\n",
    "  SELECT id, order_id, qty, books FROM (\n",
    "  VALUES \n",
    "  ('C001', 'O001', 48, ARRAY(map(\"id\",\"B01\", \"qty\",\"5\", \"ttl\",\"50\"))),\n",
    "  ('C002', 'O002', 56, ARRAY(map(\"id\",\"B02\", \"qty\",\"6\", \"ttl\",\"60\"), map(\"id\",\"B03\", \"qty\",\"3\", \"ttl\",\"30\")))  \n",
    "  )AS T(id, order_id, qty, books)\n",
    ")\n",
    "SELECT order_id, multiple_copies \n",
    "FROM(\n",
    "  SELECT order_id, books, FILTER(books, i -> i.qty >=4) AS multiple_copies\n",
    "  FROM C\n",
    ")WHERE size(multiple_copies) > 0\n",
    "\n",
    "#---------------------------------------Transform array value----------------------\n",
    "WITH C AS \n",
    "(\n",
    "  SELECT id, order_id, qty, books FROM (\n",
    "  VALUES \n",
    "  ('C001', 'O001', 48, ARRAY(map(\"id\",\"B01\", \"qty\",\"5\", \"ttl\",\"50\"))),\n",
    "  ('C002', 'O002', 56, ARRAY(map(\"id\",\"B02\", \"qty\",\"6\", \"ttl\",\"60\"), map(\"id\",\"B03\", \"qty\",\"3\", \"ttl\",\"30\")))  \n",
    "  )AS T(id, order_id, qty, books)\n",
    ")\n",
    "SELECT order_id, books, TRANSFORM(books, i -> CAST(i.qty * 0.5 AS float)) AS discount\n",
    "FROM C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL UDFs**\n",
    "\n",
    "UDF functions leverage spark sql directly maintaining all the optimization of Spark when applying our custom logic to large datasets.\n",
    "Everything is evaluated natively in Spark. So it's optimized for parallel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION get_url(email STRING)\n",
    "RETURNS STRING\n",
    "\n",
    "RETURN concat(\"https://www.\", split(email, \"@\")[1])\n",
    "\n",
    "#-----\n",
    "\n",
    "CREATE FUNCTION site_type(email STRING)\n",
    "RETURNS STRING \n",
    "RETURN CASE \n",
    "    WHEN email like \"%.com\" THEN \"Commercial business\"\n",
    "    WHEN email like \"%.org\" THEN \"non-profit Org\"\n",
    "    WHEN email like \"%.edu\" THEN \"Edu institution\"\n",
    "    ELSE concat(\"Unknown extension for domain:\", split(email, \"@\")[1])\n",
    "END;\n",
    "\n",
    "DROP FUNCTION get_url;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Data Processing\n",
    "\n",
    "- Process Streaming Data\n",
    "- DataStreamReader\n",
    "- DataStreamWriter\n",
    "\n",
    "##### Data Stream\n",
    "- Any data source that grows over time\n",
    "- New files landing in cloud storage\n",
    "- Updates to a database captured in a CDC feed\n",
    "- Events queued in a pub/sub messaging feed\n",
    "\n",
    "##### Processing Data Stream\n",
    "2 approaches\n",
    "- Reprocess the entire source dataset each time\n",
    "- Only process those new data added since last update\n",
    "    - Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structured Streaming** : Spark Structured streaming is a scalable streaming processing engine. Allows to query an infinite data source where automatically detects new data and persists the result incrementally into a data sink. *Data Sink* is just a durable file system, such as files/tables.\n",
    "\n",
    "infinite data source ====> Spark Streaming ====> data sink(treat is as table to query)\n",
    "\n",
    "**Q** How to interact and query an infinite data source?\n",
    "Treat it as a table, in-fact the maginc behind Spark Structured Streaming is that it allows users to interact with ever-growing data source as if it were just a static table of records. So new data in the input data stream is simply treated as new rows appended to a table. Such a table representing an infinite data source is seen as \"unbounded\" table.\n",
    "\n",
    "Input Streaming Table : could be a dir of files, a messaging system like Kafka, or Simply a Delta table. Delta lake is well integrated with spark structured streaming.\n",
    "\n",
    "**spark.readStream()**: to query the delta table as a stream source, which allows to process all of the data present in the table as well as any new data that arrive later. Creates a *streaming data frame* on which we can apply any transformation as if it were just a static DataFrame.  `streamDF = spark.readStream.table(\"InputTable)`\n",
    "\n",
    "**spark.writeStream()**: to persist the result of a streaming query, need to write them out to durable storage using *dataframe.writeStream() method.  \n",
    "`streamDF.writeStream.trigger(processingTime=\"2 minutes\").outputMode(\"append\").option(\"checkpointLocation\", \"/path\").table(\"output tbl\")`\n",
    "\n",
    "**Trigger Intervals**\n",
    "|Trigger|Method Call|Behavior|\n",
    "|-------|-----------|--------|\n",
    "|Unspecified| | Default: processingTime=\"500ms\"|\n",
    "|Fixed interval|.trigger(processingTime=\"5 minutes\")|process data in micro-batches at the user-specified intervals|\n",
    "|Triggered batch|.trigger(once=True)|Process all available data in a single batch, then stop\n",
    "|Triggered micro-batches|.trigger(availableNow=True)|Process all available data in multiple micro-batches, then stop|\n",
    "\n",
    "**Output Modes**\n",
    "|Mode|Method Call|Behavior|\n",
    "|----|-----------|--------|\n",
    "|Append(default|.outputMode(\"append)|Only newly appended rows are incrementally appended to the target table with each batch|\n",
    "|Complete|.outputMode(\"complete)|The target table is overwritten with each batch|\n",
    "\n",
    "**Checkpointing**\n",
    "- Store stream state\n",
    "- Track the progress of your stream processing\n",
    "- Can not be shared between separate streams\n",
    "\n",
    "**Guarantees**: Structured streaming provides 2 guarantees\n",
    "- Fault Tolerance : Checkpointing + Write-ahead logs : record the offset range of data being processed during each trigger interval.\n",
    "- Exactly-once guarantee : Idempotent sinks : enusres exactly once data processing bcoz the streaming sinks are designed to be idempotent. That is multiple writes of the same data, of course identified by the offset, do not result in duplicates being written to the sink.\n",
    "\n",
    "Above 2 guarantees here only work if the streaming source is repeatable, like cloud based object storage or pub/sub messaging service.So repeatable data sources and idempotent sinks allows spark structured streaming to ensuire end-to-end exactly once semantics under any failure condition.\n",
    "\n",
    "**UnSupported Operations**\n",
    "Some operations are not supported by streaming DataFrame\n",
    "- Sorting\n",
    "- DeDuplication\n",
    "- Advanced streaming methods like *Windowing & Watermarking* that can help to do such operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streaming Temp View\n",
    "spark.readStream.table(\"emp\").createOrReplaceTempView(\"tmp_vw_emp\")\n",
    "#Below command will intitialize stream not above \n",
    "spark.sql(\"SELECT * FROM tmp_vw_emp\")\n",
    "\n",
    "#coz we querying a streaming temp view, this becomes a streaming query that executes infinitely, \n",
    "#rather than completing after retrieving a single set of results\n",
    "spark.sql(\"SELECT empid, sum(attendance) FROM tmp_vw_emp GROUP BY empid\")\n",
    "\n",
    "#%sql\n",
    "CREATE OR REPLACE TEMP VIEW tmp_vw_emp_attendance AS\n",
    "(\n",
    "    SELECT empid, sum(attendance) FROM tmp_vw_emp GROUP BY empid\n",
    ")\n",
    "\n",
    "#Spark always loads streaming views/tables as streaming DF, and static views as static DF meaning that: incremental processing must be\n",
    "#defined from the very beginning with Read Logic to support later an incremental writing. Then, we are using DF writeStream method to\n",
    "#persist the result of a streaming query to a durable storage.\n",
    "spark.table(\"tmp_vw_emp_attendance\").writeStream\\\n",
    ".trigger(processingTime='4 seconds')\\\n",
    ".outputMode(\"Complete\")\\ #for aggregation streaming queries, must always use \"complete\" mode to overwrite the table with new calculation\n",
    ".option(\"checkpointLocation\", \"dbfs:/same_path/to/table_or_view\")\\\n",
    ".table(\"emp_attendance_summary\")\n",
    "\n",
    "\n",
    "#Insert new records in emp table\n",
    "INSERT INTO TABLE emp values (.....)\n",
    "\n",
    "#\n",
    "spark.table(\"tmp_vw_emp_attendance\").writeStream\\\n",
    ".trigger(availableNow=True)\\\n",
    "#modify the trigger method to change our query from always-on query triggered every 4 seconds to a triggered incremental batch\n",
    "#with this trigger query will process all new avail data and stop on its own after execution. In this case we can use the\n",
    "#awaitTermination method to block the execution of any cell in this notebook unitl the incremental batch's werit has succeeded\n",
    ".outputMode(\"Complete\")\\ \n",
    ".option(\"checkpointLocation\", \"dbfs:/same_path/to/table_or_view\")\\\n",
    ".table(\"emp_attendance_summary\")\n",
    ".awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incremental Data Ingestion** : Loading new data files encountered since the last ingestion, reduces redundant processing, 2 mechanism:\n",
    "- COPY INTO  \n",
    "    - SQL Command allows to load data from a file location into a Delta table\n",
    "    - Idempotently and incrementally load new data files : files already loaded are skipped\n",
    "    - `COPY INTO schema.table\n",
    "        FROM '/path/to/files'\n",
    "        FILEFORMAT=<format>\n",
    "        FORMAT_OPTIONS(<format_options>)\n",
    "        COPY_OPTIONS(<copy options>)`\n",
    "    - `COPY INTO hr.emp FROM '/path/to/files' FILEFORMAT = CSV FORMAT_OPTIONS('delimiter'='|', 'header'='true') COPY_OPTIONS('mergeSchema'='true')`\n",
    "    - When to use:\n",
    "        - Thousands of files\n",
    "        - Less efficient at scale\n",
    "- Auto Loader\n",
    "    - Strucutured Streaming\n",
    "    - Can process billions of files\n",
    "    - Support near real-time ingestion of millions of files per hour\n",
    "    - Auto loader has a specific format of StreamReader called *\"cloudFiles\"*\n",
    "    - to provide the location whee auto loader can store the schema, use the option *\"cloudFiles.schemaLocation\"*, and this location could be simply the same as the checkpoint location\n",
    "    - Auto loader is a streaming query since it uses Spark Structured Streaming to load data incrementally\n",
    "    - *Auto loader Checkpointing* : to track the ingestion proces \n",
    "        - Store metadata of the discovered files\n",
    "        - exactly-once guarantees\n",
    "        - Fault tolerance: in case of failure can auto-resume from where left off\n",
    "    - `spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", <source_format>).load('/path/to/files).writeStream.option(\"checkpointLocation\", <checkpoint_dir>).option(\"mergeSchema\", \"true\").table(<table_name>)`\n",
    "    - `spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"parquet\").option(\"cloudFiles.schemaLocation\", \"/path/to/checkpoint_dir1\").load('/path/to/files).writeStream.option(\"checkpointLocation\", \"/path/to/checkpoint_dir1\").table(\"hr.emp\")`\n",
    "    - When to use:\n",
    "        - Millions of files\n",
    "        - Efficient at scale\n",
    "        - Auto loader can split the processing into multiple batches so it is more efficient at scale.\n",
    "        - DBricks recommends to use Auto Loader as a general best practice when ingesting data from a cloud object storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-hop Architecture**\n",
    "- Medallion Architecture\n",
    "- Organize data in multi-layered approach\n",
    "- Incrementally improving the structure and quality of data as it flows through each layer\n",
    "- `(Batch, Streaming) ===> [Bronze(Ingested Raw data) ---> Silver(Filtered, Cleaned, Augmented data) ---> Gold(Aggregated Data)] ===> BI/ML`\n",
    "    - Bronze table : contains raw data ingested from various sources.like json files, operational dbs, or kafka stream\n",
    "    - Silver table : provides more refined view of our data, ex data can be cleaned and filtered at this level, and we can join fields from various brands tale to enrich our silver records\n",
    "    - Gold table : provides business-level aggregations, often used for reporting and dashboarding\n",
    "- with this architecture we incrementally improve the structure and the quality of data as it flows through each layer\n",
    "\n",
    "**Benefits**\n",
    "- Simple data model\n",
    "- Enables incremental ETL\n",
    "- Combine streaming and batch workloads in unified pipeline : so ceach stage can be configured as a batch or streaming job.\n",
    "- Can recreate your tables from raw data at any time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Pipelines\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
