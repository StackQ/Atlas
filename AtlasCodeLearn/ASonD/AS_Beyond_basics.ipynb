{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Cluster Runtime Architecture\n",
    "\n",
    "**Cluster** : Pool of computers working together but viewed as a single system\n",
    "- Example Cluster config\n",
    "    - Worker Node Capacity\n",
    "        - 16 CPU Cores\n",
    "        - 64 GB RAM\n",
    "    - Cluster Capacity\n",
    "        - 160 CPU Cores\n",
    "        - 640 GB RAM\n",
    "\n",
    "- will use the spark-submit command and submit my spark application to the cluster\n",
    "- Request will go to the YARN/Kubernetes/Mesos resource manager.\n",
    "- YARN RM wil create one *Application Master container* on a worker node and start my application's main() method in the container, here container is an isolated virtual runtime environment. Comes with some CPU and memory allocation.\n",
    "Example- let's assume YARN RM gave 4 CPU Cores and 16GB memory to this container and started it on a worker node, where this worker node has total 16CPU cores and 64GB of memory.\n",
    "- Application Master Container - The container is running the main() method of my application either in Python or Scala. But Spark is written in Scala, and it runs in the Java virtual machine. Scala is a JVM language, and it always runs in the JVM. So they created a Java wrapper on top of the Scala code. And then they crated a Python wrapper on top of the Java wrappers, and this Python wrapper is known as PySpark.\n",
    "- So here we have Python code in my main() method. This python code is designed to start a Java main() method internally. So PySpark application will start a JVM application. Once we have the JVM application, the PySpark wrapper will call the Java Wrapper using the Py4J connection. Py4J allows a Python application to call a Java application. It will always start a JVM application and call Spark APIs in the JVM. The actual Spark application is always a Scala application running in the JVM, BUT PySpark is calling Java Wrapper using Py4J, and Java Wrapper runs Scala code in the JVM.\n",
    "- PySpark main method is my PySpark Driver and JVM application here is my Application Driver, Spark application driver is the main method of our application. So for PySpark we would have PySpark Driver + Application Driver, for Scala we will have only Application Driver.\n",
    "- Spark application is a distributed application in itself, means application driver distributes the work to others. Driver doesn't perform any data processing work, instead it will create some executors and get the work done from them. After starting, the driver will go back to the YARN RM and ask for some more containers.\n",
    "- RM will create some more containers on worker nodes and give them to the driver. \n",
    "Ex: assume we got 4 new containers, each comes with 4 CPU Cores and 16 GB of memory, now the driver will start spark executor in these containers. Each container will run one SPARK executor, and the Spark executor is a JVM application. So driver is a JVM application and executor is also a JVM application. These executors are responsible for doing all the data processing work. The driver will assign work to the executors and monitor them, and manage the overall application, but the exectors do all the data processing.\n",
    "- Application Master Container[[PySpark Driver + JVM/Application Driver]] -> cluster RM[YARN] -> Workers[JVM executors, Python Worker]  \n",
    "if using some additional Python libraries that are not part of the PySpark, ex- UDFs\n",
    "- Python worker is a Python runtime environment, and need them only if using some python specific code or libraries. PySpark is python wrapper on java code. So as long as we using only PySpark, do not need Python Runtime environment, All the PySpark code is translated into Java code, and it runs in the JVM. Executors will create a Python Runtime Environment so they can execute our Python Code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Submit and Important Options\n",
    "\n",
    "**Spark Submit** : A command line tool that allows to submit the Spark application to the cluster\n",
    "ex- \n",
    "    - spark-submit --class <main class> --master <master-url> --deploy-mode <deploy-mode> <application-jar/pyspark script> \n",
    "    [application-args]  \n",
    "    - spark-submit --master yarn --deploy-mode cluster --driver-cores 2 --driver-memory 8G --num-executors 4 --executor-cores 4 --executor-memory 16G hello-spark.py  \n",
    "    - spark-submit --class myclass.something.HelloSpark --master yarn --deploy-mode cluster --driver-cores 2 --driver-memory 8G --num-executors 4 --executor-cores 4 --executor-memory 16G hello-spark.jar  \n",
    "- --class : Not applicable for PySpark\n",
    "- --master : YARN, local[3]\n",
    "- --deploy-mode : client/cluster\n",
    "- --conf : spark.executor.memoryOverhead = 0.20\n",
    "- --driver-cores : 2\n",
    "- --driver-memory : 8G\n",
    "- --num-executors : 4\n",
    "- --executor-cores : 4\n",
    "- --executor memory : 16G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Modes - Client/Cluster mode\n",
    "\n",
    "- Cluster Mode : Driver runs in the cluster, allows to submit the application and log off from the client machine, coz the driver & executors are running on the cluster, nothing active on client's machine. Runs faster coz driver is closer to executors.\n",
    "    - No dependency on client machine\n",
    "    - Performance\n",
    "- Client Mode : Driver runs in client machine, when driver is local, can easily communicate with the driver and show it back to us.\n",
    "    - spark-shell, pyspark, spark-sql : gives interacive method to work with Spark, run the code and show us the results\n",
    "    - Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Jobs - Stage, Shuffle, Task, Slots\n",
    "\n",
    "**Spark Data Frame API Categores**\n",
    "- Transformations\n",
    "    - Used for transforming data\n",
    "    - Furhter classification\n",
    "        - Narrow dependency\n",
    "            - peformed in parallel on data partitions\n",
    "            - ex- select(), filter(), withColumn(), drop()\n",
    "        - Wide dependency\n",
    "            - Pefrormed after grouping data from multiple partitions\n",
    "            - ex- groupBy(), join(), cube(), rollup() and agg(), repartition()\n",
    "- Actions \n",
    "    - Used to trigger some work(Job)\n",
    "    - Example: read(), write(), collect(), take() and count()\n",
    "    - All spark actions trigger one/more Spark jobs.\n",
    "\n",
    "- Spark will run each code block as one spark job in other words each spark action runs as a spark job that contains all the transformations of its code block, so we will look for actions to separate spark code blocks\n",
    "- BLOCK(Decide by each action occurred) :: 0 : (Read Action) \n",
    "`surveyRawDF = spark.read.option(\"header\", \"true).option(\"inferSchema\", \"true\").cav(args(0))` \n",
    "\n",
    "- BLOCK(Decide by each action occurred) :: 1 : (Collect Action) \n",
    "    - ###### Wide Transformation 1 : reparition\n",
    "    `patitioinedSurveyDF = surveyRawDF.repartition(numParitions=2)`  \n",
    "    - ###### 2(narrow transformation-Where), 3(narrow transformation - Select), 4(Wide Transformation-groupBy), 5(narrow trans-count)\n",
    "    `countDF = patitioinedSurveyDF.where(\"Age < 40\").select(col=\"Age:, cols=\"Gender\",\"Country\",\"state\").groupBy(\"Country\").count()`  \n",
    "    - ###### ACTION - collect()\n",
    "    `logger.info(countDF.collect().mkString(\"->\"))`  \n",
    "\n",
    "- **BLOCK 1** : let's consider this block \n",
    "    - Application Driver will take this block, compile it and create a Spark Job. But this job will be performed by the executors, coz the Driver doesn't perform any data processing job. Driver will break this job into smaller tasks and assign them to executors.\n",
    "    - Driver will create a *logical query plan* for each spark job.\n",
    "    - Job 2(coz job1 is to read csv above) : will start with surveyDF and repartition it to create partitionedSurveyDF and then apply 4 transformations to create the countDF  \n",
    "    *Logical Plan* :: surveyDF -> REPARTITION(yields partitionedSurveyDF) -> [ WHERE -> SELECT -> GROUP BY -> COUNT(yields countDF) ]\n",
    "    - Once we have the logical plan, the driver will start breaking this plan into stages. Driver will look at this plan to find out the WIDE Transformations. Here we have 2 Wide transformation(repartition() & groupBy() method) in CODE BLOCK 1. So the driver will break this plan after each wide transformation, and our logical plan broken down into 3 stages, each stage can have one/more narrow transformations and the last operation of the stage is a WIDE transformation. \n",
    "        - STAGE 1 :: REPARTITION\n",
    "        - STAGE 2 :: [WHERE, SELECT, GROUPBY]\n",
    "        - STAGE 3 :: COUNT\n",
    "    - Spark can't run these stages in parallel, it should finish the 1st stage, and then only can start the next stage coz the output of the first stge is an input for the next stage\n",
    "\n",
    "        - STAGE 2 :: [WHERE, SELECT, GROUPBY]\n",
    "        - STAGE 3 :: COUNT\n",
    "\n",
    "**STAGE 1** :: REPARTITION : read surveyDF and repartition it to create partitionedSurveyDF(2 partition). The final output of the stage must be stored in an *exchange buffer(WRITE EXCHANGE here)*. Output of this stage becomes the input of the next stage, so the next stage starts with an *exchange buffer(READ EXCHANGE here)*. Spark is a distributed system so both exchange may be on 2 diff/same worker nodes. So we must consider a copy of data partitions from the WRITE Exchange to Read Exchange, and this copy operation is popularly known as the Shuffle/Sort operation. Where Shuffle/Sort is not a plain copy of data, a lot of things happens here. So bascially Shuffle/Sort will move data from the WRITE Exchagne to Read Exchange. Notice stage ends with a WIDE Transformation, and hence it requires a Shuffle/Sort of the data, which is an expensive operation in the Spark Cluster and it requires a Write Exchange Buffer and a Read Exchange Buffer. The data from Write Exchange Buffer is sent to the Read Exchange Buffer over the network.\n",
    "\n",
    "TASK1[REPARITION(2)] ====> |Write Exchange(stage 1| \n",
    "\n",
    "**STAGE 2** :: [WHERE, SELECT, GROUPBY] : Read Exchange has got 2 partitions coz of above repartition(2) operation and also configured spark shuffle partitions to ensure we preserve those 2 partitions in the Read Exchange. So we have 2 input partitions here and can run these transformations in parallel on those 2 partitions, here we have 2 parallel tasks in this stage. This stage also ends with a Wide Transformation, again we need a Shuffle/Sort operation here  \n",
    "\n",
    "-------------- ====>partition 1 -> TASK1[WHERE, SELECT, GROUPBY] >  \n",
    "|Read Exchange from stage1 input|-----------------------------------------------------|Write Exchange(stage 2)|  \n",
    "-------------- ====>partition 2 -> TASK2[WHERE, SELECT, GROUPBY] >  \n",
    "\n",
    "**STAGE 3** :: COUNT : Assume 2 partitions in Stage 2's Write Exchange, so this stage's task can also run in parallel\n",
    "\n",
    "-------------- ====>partition 1 -> TASK1[COUNT] >  \n",
    "|Read Exchange from stage 2 input|  \n",
    "-------------- ====>partition 2 -> TASK2[COUNT] >  \n",
    "\n",
    "Summary:\n",
    "- Job : Spark creates 1 job for each action, may contain a series of multiple transformations. Spark engine will optimize those transformations and create a logical plan for the job.\n",
    "- Stage : Spark will break the logical plan at the end of every Wide Transformation and create 2/more stages. If no wide trans plan will be a single-stage plan, but if have N Wide Transformations, plan should have N+1 stages.\n",
    "- Shuffle/Sort : Data from one stage to another stage is shared using the shuffle/sort operation.\n",
    "- Task : Each stage may be executed as one or more parallel tasks. Task is a smallest unit of work in a Spark job. Spark Driver assigns these tasks to the executors and asks them to do the work. Executor needs the Task Code & Data Partitions to perform the task. Driver is responsible for assigning a task to the executor. Executor will ask for the code/API to be executed for the task, and also DF partition on which to execute the given code. The application driver facilitates both these things to the executor, and the executor performs the task.\n",
    "\n",
    "How Above plan will fit into the cluster?  \n",
    "Consider we have a driver and 4 executors(on 4 worker nodes), each executor will have one JVM process. But we assigned 4 CPU Cores to each executor with 16GB memory, so my Executor-JVM can create 4 parallel threads on each worker node and that's the ***SLOT*** capacity of my executor. So each executor can have 4 parallel threads, and we call them executor slots. The driver knows how many slots do we have at each executor, and it is going to assign tasks to fit in the executor slots.\n",
    "\n",
    "client Machine(spark-submit cluster Mode) -> YARN RM -> Driver JVM -[4 Workers with thier own Executor-JVM(each with 4 slots)]\n",
    "\n",
    "Collect() Action : requires each task to send data back to driver. So the tasks of the last stage will send the result back to the driver over the network. The driver will collect data from all the tasks and present it to you. If we have an action to write the result in a data file, in that case all the tasks will write a data file partition and send the partition details to the driver. Driver considers the job done when all the tasks are successful. If any tasks fails, the driver might want to retry it. It can restart the task at a diff executor. If all retries also fail, then the driver returns an exception and marks the job failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark SQL Engine and Query Planning\n",
    "\n",
    "<img src=\"/workspaces/Atlas/DSA/images/query_planning.jpeg\">\n",
    "\n",
    "- Unresolved Logical Plan : in-case of DataFrame API code, spark will take action and all it's preceding transformations to consider it a single job. Similarly if have a SQL expression, Spark considers 1 SQL expression as one Job. Each job represents a *logical query plan*. This 1st logical plan is a user-created logical plan.\n",
    "- Spark SQL Engine : unresolved logical plan goes to Spark SQL Engine whether DF API/SQL. For spark they are nothing but a Spark Job represented as a logical plan. Spark SQL Engine will process logical plan in 4 stages  \n",
    "Ex- SELECT prod_qty + 5 FROM sales\n",
    "    - Analysis : parse code for errors and incorrect names using catalog, might apply an implicit casting to the 'prod_qty' to perform and validate the addition operation. So analysis phase will parse code and create a fully resolved plan.\n",
    "    - Logical Optimization : Applies standard rule-based optimizations to the logical plan. Ex-\n",
    "        - Constant folding\n",
    "        - Predicate pushdown\n",
    "        - Partition pruning\n",
    "        - Null propagation\n",
    "        - Boolean expression simplification\n",
    "    - Physical Planning(Physical plans + cost model + selected physical plan) : Takes a logical plan and generates one/more physical plans in this phase. Physcial planning applies cost-based optimization, calculate each plan's cost and finally select the plan with the least cost. At this stage, mostly use diff join algorithms to create more than one physcial plan. For ex they might create 1st plan using Broadcast join and another using Sort merge and more using shuffle hash join. Then apply a cost to each plan and choose the best one.\n",
    "    - Code Generation : Best physical plan goes into code generation, and engine will generate Java byte code for the RDD operations in the physical plan. And that's why Spark is also said to act as a compiler, coz it uses start of the art compiler technology for code generation to speed up execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
